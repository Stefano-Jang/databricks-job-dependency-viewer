{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1d4adc-6267-4c04-ae9e-a1d4900476d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Subject:** Comprehensive Lakeflow Declaritive Pipelines (formerly \"DLT\") Monitoring Solution - Unified Analytics Across Your Databricks Environment\n",
    "</br>\n",
    "**Crafted By:** Alex Linke, Sr. Solutions Architect @ Databricks\n",
    "\n",
    "\n",
    "## üéØ **The Challenge You're Facing**\n",
    "\n",
    "Currently, your DLT pipelines create scattered `dlt_event_log_*` tables across different catalogs and schemas throughout your Databricks environment. This makes it nearly impossible to:\n",
    "\n",
    "- Get a **holistic view** of pipeline performance across your entire organization\n",
    "- Track **individual pipeline runs** with start times, durations, and success rates\n",
    "- Identify **performance trends** and bottlenecks before they impact business operations\n",
    "- Create **centralized monitoring** and alerting for your data engineering teams\n",
    "- Answer simple questions like *\"Which pipelines are failing most often?\"* or *\"Why did our ETL take 3 hours yesterday?\"*\n",
    "\n",
    "## ‚ú® **Our Solution: DLT Pipeline Analytics Consolidator**\n",
    "\n",
    "We've built a comprehensive **PySpark-based solution** that automatically:\n",
    "\n",
    "üîç **Discovers** all DLT event log tables across your entire Databricks environment  \n",
    "üîÑ **Consolidates** scattered logs into a unified, queryable dataset  \n",
    "üìà **Extracts** real pipeline metadata (names, run IDs, clusters, durations)  \n",
    "üìä **Generates** production-ready analytics tables with rich insights  \n",
    "üíæ **Saves** everything as Delta tables for ongoing monitoring and dashboards  \n",
    "\n",
    "## üöÄ **Immediate Business Value**\n",
    "\n",
    "### **Operational Visibility**\n",
    "- **Real-time monitoring** of all pipeline runs across your environment\n",
    "- **Success/failure rates** by pipeline name with drill-down capabilities\n",
    "- **Performance trends** to identify degradation before it impacts SLAs\n",
    "\n",
    "### **Cost Optimization**\n",
    "- **Duration analysis** to identify long-running pipelines consuming excessive compute\n",
    "- **Cluster utilization** insights to optimize resource allocation\n",
    "- **Error pattern analysis** to reduce troubleshooting time\n",
    "\n",
    "### **Data Quality Assurance**\n",
    "- **Failed run tracking** with detailed error messages and affected datasets\n",
    "- **Trend analysis** to predict and prevent future pipeline issues\n",
    "- **Compliance reporting** with full audit trails of all pipeline executions\n",
    "\n",
    "## üìä **What You'll Get - Concrete Deliverables**\n",
    "\n",
    "### **1. Executive Dashboard Data**\n",
    "```sql\n",
    "-- Pipeline Health Overview\n",
    "SELECT pipeline_name, total_runs, success_rate, avg_duration_minutes\n",
    "FROM your_catalog.analytics.dlt_performance_metrics\n",
    "WHERE success_rate < 0.95 OR avg_duration_minutes > 60\n",
    "```\n",
    "\n",
    "### **2. Operational Monitoring Tables**\n",
    "- **`dlt_pipeline_runs`** - Every pipeline execution with start/end times, status, duration\n",
    "- **`dlt_performance_metrics`** - Success rates, average durations, error patterns by pipeline\n",
    "- **`dlt_error_analysis`** - Detailed failure analysis with error messages and affected runs\n",
    "- **`dlt_duration_trends`** - Performance trends with median, P95, and outlier detection\n",
    "\n",
    "### **3. Real-time Analytics Examples**\n",
    "- *\"Show me all failed pipeline runs in the last 7 days\"*\n",
    "- *\"Which pipelines have the highest average duration?\"*\n",
    "- *\"What's our overall pipeline success rate by day?\"*\n",
    "- *\"Which clusters are most/least reliable for specific pipelines?\"*\n",
    "\n",
    "## ‚ö° **Implementation & Time to Value**\n",
    "\n",
    "### **Quick Deployment**\n",
    "- **Single PySpark script** - runs directly in your Databricks environment\n",
    "- **No external dependencies** - uses your existing Databricks infrastructure\n",
    "- **Configurable scope** - can start with specific catalogs/schemas and expand\n",
    "- **Immediate results** - see analytics within minutes of first run\n",
    "\n",
    "### **Minimal Maintenance**\n",
    "- **Automated discovery** - finds new DLT tables as you create them\n",
    "- **Scheduled execution** - can run daily/weekly via Databricks Jobs\n",
    "- **Self-maintaining** - handles schema changes and new pipeline deployments\n",
    "- **Scalable** - performs well across environments with hundreds of pipelines\n",
    "\n",
    "## üíº **Business Impact - What This Means for You**\n",
    "\n",
    "### **For Data Engineering Teams**\n",
    "- **Reduce troubleshooting time** from hours to minutes with centralized error analysis\n",
    "- **Proactive monitoring** prevents pipeline failures from impacting downstream processes\n",
    "- **Performance optimization** identifies bottlenecks before they affect SLAs\n",
    "\n",
    "### **For Data Platform Teams**\n",
    "- **Resource optimization** through cluster usage and duration analysis\n",
    "- **Capacity planning** with trend analysis and growth projections\n",
    "- **Compliance reporting** with complete audit trails\n",
    "\n",
    "### **For Business Leadership**\n",
    "- **Data reliability metrics** - know your data pipeline health at a glance\n",
    "- **Cost visibility** - understand compute spending across all data pipelines\n",
    "- **Risk mitigation** - early warning system for data pipeline issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a473ee50-5c36-427b-935c-5a5bfda97414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple DLT Event Log Consolidator for Databricks Notebooks\n",
    "\n",
    "A focused  script to discover, consolidate, and analyze DLT event logs\n",
    "across your Databricks environment. Run this directly in a notebook cell.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import percentile_approx, element_at\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURATION - Modify these settings as needed\n",
    "# ================================================================================\n",
    "\n",
    "DAYS_BACK = int(dbutils.widgets.get(\"ldp_error_table.days_back\").strip())\n",
    "temp_source_catalogs = dbutils.widgets.get(\"ldp_error_table.source_catalogs\")\n",
    "SOURCE_CATALOGS = [x.strip() for x in temp_source_catalogs.split(\",\") if x.strip()]\n",
    "OUTPUT_CATALOG = dbutils.widgets.get(\"ldp_error_table.output_catalog\").strip()\n",
    "OUTPUT_SCHEMA = dbutils.widgets.get(\"ldp_error_table.output_schema\").strip()\n",
    "\n",
    "CONFIG = {\n",
    "    # Discovery settings\n",
    "    'include_system_catalogs': False,\n",
    "    #'target_catalogs': ['alex_linke', 'jordan_reiser', 'abhijeet_kumar', 'dbdemos_abhijeet_kumar', 'mfg_sandbox'],  # Only scan these catalogs\n",
    "    'target_catalogs': SOURCE_CATALOGS,\n",
    "    'catalog_filter': None,  # e.g., 'prod.*' to only scan prod catalogs (regex)\n",
    "    'schema_filter': None,   # e.g., 'dlt.*' to only scan DLT schemas (regex)\n",
    "    \n",
    "    # Data processing\n",
    "    'days_back': DAYS_BACK,  # Only process last N days (None for all data)\n",
    "    'add_source_info': True,  # Add columns showing source table info\n",
    "    \n",
    "    # Output settings\n",
    "    'save_results': True,\n",
    "    'output_catalog': OUTPUT_CATALOG, #This will need to be modified\n",
    "    'output_schema': OUTPUT_SCHEMA, # This will need to be modified\n",
    "    'show_sample_data': True,\n",
    "    'max_rows_to_show': 20 #You can change this if you want, its just for the preview when the script is runnign\n",
    "}\n",
    "\n",
    "print(\"üöÄ DLT Event Log Consolidator - Starting...\")\n",
    "print(f\"Configuration: {CONFIG}\")\n",
    "\n",
    "# ================================================================================\n",
    "# STEP 1: DISCOVER DLT EVENT LOG TABLES\n",
    "# ================================================================================\n",
    "\n",
    "def discover_dlt_tables(spark, config):\n",
    "    \"\"\"Discover all DLT event log tables across the environment\"\"\"\n",
    "    \n",
    "    print(\"\\nüîç STEP 1: Discovering DLT event log tables...\")\n",
    "    \n",
    "    discovered_tables = []\n",
    "    \n",
    "    # Get catalogs to scan\n",
    "    try:\n",
    "        # Use specific target catalogs if provided, otherwise discover all\n",
    "        if config.get('target_catalogs') and config.get('target_catalogs')[0].lower() != 'all':\n",
    "            catalogs = config['target_catalogs']\n",
    "            print(f\"üìÇ Using target catalogs: {catalogs}\")\n",
    "        else:\n",
    "            catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "            catalogs = [row.catalog for row in catalogs_df.collect()]\n",
    "            \n",
    "            # Filter system catalogs\n",
    "            if not config['include_system_catalogs']:\n",
    "                system_catalogs = ['system', 'information_schema', 'samples']\n",
    "                catalogs = [cat for cat in catalogs if cat not in system_catalogs]\n",
    "            \n",
    "            # Apply catalog filter if specified\n",
    "            if config['catalog_filter']:\n",
    "                catalogs = [cat for cat in catalogs if re.match(config['catalog_filter'], cat)]\n",
    "            \n",
    "            print(f\"üìÇ Discovered {len(catalogs)} catalogs: {catalogs}\")\n",
    "        \n",
    "        print(f\"üéØ Scanning {len(catalogs)} catalogs total\")\n",
    "        \n",
    "        for i, catalog in enumerate(catalogs, 1):\n",
    "            catalog_start_time = time.time()\n",
    "            print(f\"\\nüìÇ [{i}/{len(catalogs)}] Processing catalog: {catalog}\")\n",
    "            try:\n",
    "                # Get schemas in this catalog - handle different Databricks versions\n",
    "                try:\n",
    "                    schemas_df = spark.sql(f\"SHOW SCHEMAS IN {catalog}\")\n",
    "                    \n",
    "                    # Check what columns are available and use the right one\n",
    "                    columns = schemas_df.columns\n",
    "                    print(f\"    üìÅ Schema columns available: {columns}\")\n",
    "                    \n",
    "                    if 'namespace' in columns:\n",
    "                        schemas = [row.namespace for row in schemas_df.collect()]\n",
    "                    elif 'schemaName' in columns:\n",
    "                        schemas = [row.schemaName for row in schemas_df.collect()]\n",
    "                    elif 'databaseName' in columns:\n",
    "                        schemas = [row.databaseName for row in schemas_df.collect()]\n",
    "                    else:\n",
    "                        # Use first column as fallback\n",
    "                        col_name = columns[0]\n",
    "                        schemas = [getattr(row, col_name) for row in schemas_df.collect()]\n",
    "                        \n",
    "                except Exception as schema_error:\n",
    "                    print(f\"    ‚ö†Ô∏è Error getting schemas for {catalog}: {schema_error}\")\n",
    "                    # Fallback: try using information_schema\n",
    "                    try:\n",
    "                        schemas_df = spark.sql(f\"\"\"\n",
    "                            SELECT DISTINCT schema_name \n",
    "                            FROM {catalog}.information_schema.schemata\n",
    "                        \"\"\")\n",
    "                        schemas = [row.schema_name for row in schemas_df.collect()]\n",
    "                    except:\n",
    "                        schemas = []\n",
    "                \n",
    "                # Apply schema filter if specified\n",
    "                if config['schema_filter']:\n",
    "                    schemas = [schema for schema in schemas if re.match(config['schema_filter'], schema)]\n",
    "                \n",
    "                for schema in schemas:\n",
    "                    try:\n",
    "                        # Get tables in this schema - handle different column names\n",
    "                        tables_df = spark.sql(f\"SHOW TABLES IN {catalog}.{schema}\")\n",
    "                        \n",
    "                        # Check what columns are available\n",
    "                        columns = tables_df.columns\n",
    "                        \n",
    "                        if 'tableName' in columns:\n",
    "                            tables = [row.tableName for row in tables_df.collect()]\n",
    "                        elif 'name' in columns:\n",
    "                            tables = [row.name for row in tables_df.collect()]\n",
    "                        elif 'table' in columns:\n",
    "                            tables = [row.table for row in tables_df.collect()]\n",
    "                        else:\n",
    "                            # Use first column as fallback\n",
    "                            col_name = columns[0]\n",
    "                            tables = [getattr(row, col_name) for row in tables_df.collect()]\n",
    "                        \n",
    "                        # Find event log tables\n",
    "                        event_log_tables = [\n",
    "                            table for table in tables \n",
    "                            if any(pattern in table.lower() for pattern in ['event_log', 'dlt_event_log'])\n",
    "                        ]\n",
    "                        \n",
    "                        for table in event_log_tables:\n",
    "                            full_name = f\"{catalog}.{schema}.{table}\"\n",
    "                            try:\n",
    "                                # Get basic info\n",
    "                                count_result = spark.sql(f\"SELECT COUNT(*) as cnt FROM {full_name}\").collect()\n",
    "                                row_count = count_result[0].cnt if count_result else 0\n",
    "                                \n",
    "                                discovered_tables.append({\n",
    "                                    'catalog': catalog,\n",
    "                                    'schema': schema,\n",
    "                                    'table': table,\n",
    "                                    'full_name': full_name,\n",
    "                                    'row_count': row_count\n",
    "                                })\n",
    "                                \n",
    "                                print(f\"  ‚úÖ Found: {full_name} ({row_count:,} rows)\")\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(f\"  ‚ö†Ô∏è Could not access {full_name}: {str(e)[:100]}\")\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è Could not scan schema {catalog}.{schema}: {str(e)[:100]}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Could not scan catalog {catalog}: {str(e)[:100]}\")\n",
    "            \n",
    "            finally:\n",
    "                catalog_time = time.time() - catalog_start_time\n",
    "                print(f\"  ‚è±Ô∏è Catalog {catalog} completed in {catalog_time:.1f} seconds\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during discovery: {e}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nüéâ Discovery complete! Found {len(discovered_tables)} DLT event log tables\")\n",
    "    return discovered_tables\n",
    "\n",
    "# Run discovery\n",
    "discovered_tables = discover_dlt_tables(spark, CONFIG)\n",
    "\n",
    "if not discovered_tables:\n",
    "    print(\"‚ùå No DLT event log tables found. Check your permissions and DLT pipeline setup.\")\n",
    "else:\n",
    "    # Show discovery summary\n",
    "    discovery_df = spark.createDataFrame([\n",
    "        (t['catalog'], t['schema'], t['table'], t['full_name'], t['row_count']) \n",
    "        for t in discovered_tables\n",
    "    ], ['catalog', 'schema', 'table', 'full_name', 'row_count'])\n",
    "    \n",
    "    print(\"\\nüìä Discovered Tables Summary:\")\n",
    "    discovery_df.orderBy('catalog', 'schema', 'table').show(truncate=False)\n",
    "    \n",
    "    print(\"\\nüìà Summary by Catalog:\")\n",
    "    discovery_df.groupBy('catalog') \\\n",
    "                .agg(count('*').alias('table_count'), \n",
    "                     sum('row_count').alias('total_rows')) \\\n",
    "                .orderBy('catalog').show()\n",
    "\n",
    "# ================================================================================\n",
    "# STEP 2: CONSOLIDATE EVENT LOGS\n",
    "# ================================================================================\n",
    "\n",
    "def consolidate_event_logs(spark, discovered_tables, config):\n",
    "    \"\"\"Consolidate all discovered event logs into a single DataFrame\"\"\"\n",
    "    \n",
    "    print(\"\\nüîÑ STEP 2: Consolidating event log data...\")\n",
    "    \n",
    "    if not discovered_tables:\n",
    "        print(\"‚ùå No tables to consolidate\")\n",
    "        return None\n",
    "    \n",
    "    consolidated_dfs = []\n",
    "    \n",
    "    for table_info in discovered_tables:\n",
    "        table_name = table_info['full_name']\n",
    "        \n",
    "        try:\n",
    "            print(f\"  üì• Processing {table_name}...\")\n",
    "            df = spark.table(table_name)\n",
    "            \n",
    "            # Add source information if requested\n",
    "            if config['add_source_info']:\n",
    "                df = df.withColumn(\"source_catalog\", lit(table_info['catalog'])) \\\n",
    "                       .withColumn(\"source_schema\", lit(table_info['schema'])) \\\n",
    "                       .withColumn(\"source_table\", lit(table_info['table'])) \\\n",
    "                       .withColumn(\"source_full_name\", lit(table_name))\n",
    "            \n",
    "            # Apply time filtering if specified\n",
    "            if config['days_back']:\n",
    "                cutoff_date = datetime.now() - timedelta(days=config['days_back'])\n",
    "                df = df.filter(col(\"timestamp\") >= lit(cutoff_date))\n",
    "                print(f\"    üìÖ Filtered to last {config['days_back']} days\")\n",
    "            \n",
    "            # Add processing metadata\n",
    "            df = df.withColumn(\"consolidated_at\", current_timestamp())\n",
    "            \n",
    "            consolidated_dfs.append(df)\n",
    "            print(f\"    ‚úÖ Added {df.count():,} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Error processing {table_name}: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    if not consolidated_dfs:\n",
    "        print(\"‚ùå No tables could be successfully processed\")\n",
    "        return None\n",
    "    \n",
    "    # Union all DataFrames\n",
    "    print(f\"\\nüîó Combining {len(consolidated_dfs)} datasets...\")\n",
    "    consolidated_df = consolidated_dfs[0]\n",
    "    for df in consolidated_dfs[1:]:\n",
    "        consolidated_df = consolidated_df.unionByName(df, allowMissingColumns=True)\n",
    "    \n",
    "    # Get row count (cache not supported on serverless)\n",
    "    total_rows = consolidated_df.count()\n",
    "    \n",
    "    print(f\"üéâ Consolidation complete! Total rows: {total_rows:,}\")\n",
    "    return consolidated_df\n",
    "\n",
    "# Run consolidation\n",
    "consolidated_df = consolidate_event_logs(spark, discovered_tables, CONFIG)\n",
    "\n",
    "if consolidated_df:\n",
    "    print(f\"\\nüìã Consolidated Data Schema:\")\n",
    "    consolidated_df.printSchema()\n",
    "    \n",
    "    if CONFIG['show_sample_data']:\n",
    "        print(f\"\\nüìù Sample Data (showing {CONFIG['max_rows_to_show']} rows):\")\n",
    "        consolidated_df.select(\n",
    "            \"timestamp\", \"level\", \"event_type\", \"source_full_name\", \"message\"\n",
    "        ).orderBy(desc(\"timestamp\")).show(CONFIG['max_rows_to_show'], truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# STEP 3: BASIC ANALYTICS\n",
    "# ================================================================================\n",
    "\n",
    "def generate_analytics(df):\n",
    "    \"\"\"Generate comprehensive analytics by extracting pipeline metadata from event logs\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà STEP 3: Generating Advanced Pipeline Analytics...\")\n",
    "    \n",
    "    # Extract pipeline metadata from the origin struct\n",
    "    enriched_df = df.withColumn(\"pipeline_id\", col(\"origin.pipeline_id\")) \\\n",
    "                    .withColumn(\"pipeline_name\", col(\"origin.pipeline_name\")) \\\n",
    "                    .withColumn(\"pipeline_type\", col(\"origin.pipeline_type\")) \\\n",
    "                    .withColumn(\"update_id\", col(\"origin.update_id\")) \\\n",
    "                    .withColumn(\"flow_id\", col(\"origin.flow_id\")) \\\n",
    "                    .withColumn(\"flow_name\", col(\"origin.flow_name\")) \\\n",
    "                    .withColumn(\"cluster_id\", col(\"origin.cluster_id\")) \\\n",
    "                    .withColumn(\"batch_id\", col(\"origin.batch_id\")) \\\n",
    "                    .withColumn(\"request_id\", col(\"origin.request_id\")) \\\n",
    "                    .filter(col(\"pipeline_id\").isNotNull())  # Only events with pipeline metadata\n",
    "    \n",
    "    print(f\"üìä Extracted metadata from {enriched_df.count():,} events with pipeline information\")\n",
    "    \n",
    "    # 1. Detailed Pipeline Runs Analysis (grouped by update_id for actual runs)\n",
    "    print(\"\\nüèÉ Detailed Pipeline Runs Analysis:\")\n",
    "    pipeline_runs = enriched_df.groupBy(\n",
    "        \"pipeline_id\", \"pipeline_name\", \"update_id\", \"source_full_name\"\n",
    "    ).agg(\n",
    "        min(\"timestamp\").alias(\"run_start\"),\n",
    "        max(\"timestamp\").alias(\"run_end\"), \n",
    "        to_date(min(\"timestamp\")).alias(\"run_date\"),\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"event_type\").alias(\"unique_event_types\"),\n",
    "        sum(when(col(\"level\") == \"ERROR\", 1).otherwise(0)).alias(\"error_count\"),\n",
    "        sum(when(col(\"level\") == \"WARN\", 1).otherwise(0)).alias(\"warning_count\"),\n",
    "        sum(when(col(\"level\") == \"INFO\", 1).otherwise(0)).alias(\"info_count\"),\n",
    "        collect_set(when(col(\"level\") == \"ERROR\", col(\"message\"))).alias(\"error_messages\"),\n",
    "        first(\"cluster_id\").alias(\"cluster_id\"),\n",
    "        first(\"pipeline_type\").alias(\"pipeline_type\")\n",
    "    ).withColumn(\n",
    "        \"duration_minutes\",\n",
    "        (unix_timestamp(\"run_end\") - unix_timestamp(\"run_start\")) / 60\n",
    "    ).withColumn(\n",
    "        \"has_errors\", \n",
    "        col(\"error_count\") > 0\n",
    "    ).withColumn(\n",
    "        \"status\",\n",
    "        when(col(\"error_count\") > 0, \"FAILED\")\n",
    "        .when(col(\"total_events\") < 5, \"INCOMPLETE\") \n",
    "        .otherwise(\"SUCCESS\")\n",
    "    ).filter(col(\"duration_minutes\") > 0.1)  # Filter out very short spurious runs\n",
    "    \n",
    "    print(f\"üìã Found {pipeline_runs.count()} distinct pipeline runs\")\n",
    "    pipeline_runs.select(\n",
    "        \"pipeline_name\", \"run_date\", \"run_start\", \"duration_minutes\", \n",
    "        \"status\", \"error_count\", \"total_events\", \"cluster_id\"\n",
    "    ).orderBy(desc(\"run_start\")).show(20, truncate=False)\n",
    "    \n",
    "    # 2. Performance Metrics by Actual Pipeline Name\n",
    "    print(\"\\n‚ö° Performance Metrics by Pipeline:\")\n",
    "    performance_metrics = pipeline_runs.groupBy(\"pipeline_name\", \"pipeline_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_runs\"),\n",
    "            avg(\"duration_minutes\").alias(\"avg_duration_minutes\"),\n",
    "            min(\"duration_minutes\").alias(\"min_duration_minutes\"),\n",
    "            max(\"duration_minutes\").alias(\"max_duration_minutes\"),\n",
    "            stddev(\"duration_minutes\").alias(\"duration_stddev\"),\n",
    "            sum(\"error_count\").alias(\"total_errors\"),\n",
    "            sum(\"warning_count\").alias(\"total_warnings\"),\n",
    "            sum(when(col(\"status\") == \"SUCCESS\", 1).otherwise(0)).alias(\"successful_runs\"),\n",
    "            sum(when(col(\"status\") == \"FAILED\", 1).otherwise(0)).alias(\"failed_runs\"),\n",
    "            countDistinct(\"cluster_id\").alias(\"unique_clusters\"),\n",
    "            max(\"run_date\").alias(\"last_run_date\"),\n",
    "            avg(\"total_events\").alias(\"avg_events_per_run\")\n",
    "        ).withColumn(\n",
    "            \"success_rate\",\n",
    "            col(\"successful_runs\") / col(\"total_runs\")\n",
    "        ).withColumn(\n",
    "            \"failure_rate\", \n",
    "            col(\"failed_runs\") / col(\"total_runs\")\n",
    "        ).withColumn(\n",
    "            \"avg_duration_hours\",\n",
    "            col(\"avg_duration_minutes\") / 60\n",
    "        )\n",
    "    \n",
    "    performance_metrics.select(\n",
    "        \"pipeline_name\", \"total_runs\", \"success_rate\", \"avg_duration_minutes\", \n",
    "        \"max_duration_minutes\", \"last_run_date\", \"unique_clusters\"\n",
    "    ).orderBy(desc(\"total_runs\")).show(truncate=False)\n",
    "    \n",
    "    # 3. Detailed Error Analysis by Pipeline\n",
    "    print(\"\\nüîç Error Analysis by Pipeline:\")\n",
    "    error_analysis = enriched_df.filter(col(\"level\") == \"ERROR\") \\\n",
    "        .groupBy(\"pipeline_name\", \"pipeline_id\", \"event_type\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"error_count\"),\n",
    "            countDistinct(\"update_id\").alias(\"affected_runs\"),\n",
    "            min(\"timestamp\").alias(\"first_error\"),\n",
    "            max(\"timestamp\").alias(\"last_error\"),\n",
    "            collect_list(\"message\").alias(\"sample_error_messages\")\n",
    "        ).withColumn(\n",
    "            \"sample_message\",\n",
    "            element_at(col(\"sample_error_messages\"), 1)\n",
    "        ).orderBy(desc(\"error_count\"))\n",
    "    \n",
    "    error_analysis.select(\n",
    "        \"pipeline_name\", \"event_type\", \"error_count\", \"affected_runs\", \n",
    "        \"first_error\", \"last_error\", \"sample_message\"\n",
    "    ).show(15, truncate=False)\n",
    "    \n",
    "    # 4. Daily Pipeline Activity (last 7 days)\n",
    "    print(\"\\nüìÖ Daily Pipeline Activity (Last 7 Days):\")\n",
    "    recent_activity = pipeline_runs.filter(col(\"run_date\") >= date_sub(current_date(), 7)) \\\n",
    "        .groupBy(\"run_date\", \"pipeline_name\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_runs\"),\n",
    "            sum(when(col(\"status\") == \"SUCCESS\", 1).otherwise(0)).alias(\"successful_runs\"),\n",
    "            sum(when(col(\"status\") == \"FAILED\", 1).otherwise(0)).alias(\"failed_runs\"), \n",
    "            avg(\"duration_minutes\").alias(\"avg_duration\"),\n",
    "            sum(\"total_events\").alias(\"total_events\")\n",
    "        ).withColumn(\n",
    "            \"success_rate_daily\",\n",
    "            col(\"successful_runs\") / col(\"total_runs\")\n",
    "        ).orderBy(desc(\"run_date\"), \"pipeline_name\")\n",
    "    \n",
    "    recent_activity.show(truncate=False)\n",
    "    \n",
    "    # 5. Pipeline Run Duration Trends\n",
    "    print(\"\\n‚è±Ô∏è Pipeline Duration Trends (Longest Running Pipelines):\")\n",
    "    duration_trends = pipeline_runs.filter(col(\"duration_minutes\") > 1) \\\n",
    "        .groupBy(\"pipeline_name\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"runs\"),\n",
    "            avg(\"duration_minutes\").alias(\"avg_minutes\"),\n",
    "            max(\"duration_minutes\").alias(\"max_minutes\"),\n",
    "            min(\"duration_minutes\").alias(\"min_minutes\"),\n",
    "            percentile_approx(\"duration_minutes\", 0.5).alias(\"median_minutes\"),\n",
    "            percentile_approx(\"duration_minutes\", 0.95).alias(\"p95_minutes\")\n",
    "        ).orderBy(desc(\"avg_minutes\"))\n",
    "    \n",
    "    duration_trends.show(truncate=False)\n",
    "    \n",
    "    # 6. Cluster Usage Analysis\n",
    "    print(\"\\nüñ•Ô∏è Cluster Usage by Pipeline:\")\n",
    "    cluster_usage = pipeline_runs.groupBy(\"pipeline_name\", \"cluster_id\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"runs_on_cluster\"),\n",
    "            avg(\"duration_minutes\").alias(\"avg_duration_on_cluster\"),\n",
    "            sum(when(col(\"status\") == \"SUCCESS\", 1).otherwise(0)).alias(\"successful_runs\")\n",
    "        ).withColumn(\n",
    "            \"success_rate_cluster\",\n",
    "            col(\"successful_runs\") / col(\"runs_on_cluster\")\n",
    "        ).orderBy(\"pipeline_name\", desc(\"runs_on_cluster\"))\n",
    "    \n",
    "    cluster_usage.show(truncate=False)\n",
    "    \n",
    "    return {\n",
    "        'pipeline_runs': pipeline_runs,\n",
    "        'performance_metrics': performance_metrics,\n",
    "        'error_analysis': error_analysis,\n",
    "        'recent_activity': recent_activity,\n",
    "        'duration_trends': duration_trends,\n",
    "        'cluster_usage': cluster_usage\n",
    "    }\n",
    "\n",
    "# Generate analytics if we have data\n",
    "if consolidated_df:\n",
    "    analytics_results = generate_analytics(consolidated_df)\n",
    "\n",
    "# ================================================================================\n",
    "# STEP 4: SAVE RESULTS (OPTIONAL)\n",
    "# ================================================================================\n",
    "\n",
    "if consolidated_df and CONFIG['save_results']:\n",
    "    print(f\"\\nüíæ STEP 4: Saving Results...\")\n",
    "    \n",
    "    catalog = CONFIG['output_catalog']\n",
    "    schema = CONFIG['output_schema']\n",
    "    \n",
    "    try:\n",
    "        # Ensure schema exists\n",
    "        try:\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "            print(f\"‚úÖ Schema {catalog}.{schema} ready\")\n",
    "        except Exception as schema_error:\n",
    "            print(f\"‚ö†Ô∏è Could not create schema (may already exist): {schema_error}\")\n",
    "        \n",
    "        # Save consolidated data\n",
    "        consolidated_table = f\"{catalog}.{schema}.dlt_consolidated_events\"\n",
    "        print(f\"üìä Saving consolidated data to table: {consolidated_table}\")\n",
    "        consolidated_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(consolidated_table)\n",
    "        \n",
    "        # Save analytics if available\n",
    "        if 'analytics_results' in locals():\n",
    "            for name, df_result in analytics_results.items():\n",
    "                table_name = f\"{catalog}.{schema}.dlt_{name}\"\n",
    "                print(f\"üìà Saving {name} to table: {table_name}\")\n",
    "                df_result.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .saveAsTable(table_name)\n",
    "        \n",
    "        print(\"‚úÖ All results saved successfully!\")\n",
    "        print(f\"\\nüéØ Access your data:\")\n",
    "        print(f\"   Consolidated Events: SELECT * FROM {consolidated_table}\")\n",
    "        print(f\"   Pipeline Runs: SELECT * FROM {catalog}.{schema}.dlt_pipeline_runs\")\n",
    "        print(f\"   Performance Metrics: SELECT * FROM {catalog}.{schema}.dlt_performance_metrics\")\n",
    "        print(f\"   Error Analysis: SELECT * FROM {catalog}.{schema}.dlt_error_analysis\")\n",
    "        print(f\"   Recent Activity: SELECT * FROM {catalog}.{schema}.dlt_recent_activity\")\n",
    "        print(f\"   Duration Trends: SELECT * FROM {catalog}.{schema}.dlt_duration_trends\")\n",
    "        print(f\"   Cluster Usage: SELECT * FROM {catalog}.{schema}.dlt_cluster_usage\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "\n",
    "# ================================================================================\n",
    "# STEP 5: CREATE TEMPORARY VIEWS FOR SQL ACCESS\n",
    "# ================================================================================\n",
    "\n",
    "if consolidated_df:\n",
    "    print(f\"\\nüéØ STEP 5: Creating Temporary Views for SQL Access...\")\n",
    "    \n",
    "    try:\n",
    "        # Create main consolidated view\n",
    "        consolidated_df.createOrReplaceTempView(\"dlt_consolidated_events\")\n",
    "        print(\"‚úÖ Created view: dlt_consolidated_events\")\n",
    "        \n",
    "        # Create analytics views if available\n",
    "        if 'analytics_results' in locals():\n",
    "            for name, df_result in analytics_results.items():\n",
    "                view_name = f\"dlt_{name}\"\n",
    "                df_result.createOrReplaceTempView(view_name)\n",
    "                print(f\"‚úÖ Created view: {view_name}\")\n",
    "        \n",
    "        print(f\"\\nüìù You can now use SQL queries like:\")\n",
    "        print(f\"   %sql SELECT * FROM dlt_consolidated_events WHERE level = 'ERROR' ORDER BY timestamp DESC LIMIT 10\")\n",
    "        print(f\"   %sql SELECT pipeline_name, total_runs, success_rate, avg_duration_minutes FROM dlt_performance_metrics ORDER BY avg_duration_minutes DESC\")\n",
    "        print(f\"   %sql SELECT pipeline_name, status, duration_minutes FROM dlt_pipeline_runs ORDER BY run_start DESC LIMIT 20\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating views: {e}\")\n",
    "\n",
    "# ================================================================================\n",
    "# SUMMARY\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"üéâ DLT EVENT LOG CONSOLIDATION COMPLETE!\")\n",
    "print(f\"=\"*80)\n",
    "\n",
    "if discovered_tables:\n",
    "    print(f\"üìä Discovered: {len(discovered_tables)} DLT event log tables\")\n",
    "    \n",
    "if consolidated_df:\n",
    "    print(f\"üîó Consolidated: {consolidated_df.count():,} total events\")\n",
    "    print(f\"üìÖ Time Range: {CONFIG['days_back']} days back\" if CONFIG['days_back'] else \"üìÖ Time Range: All available data\")\n",
    "    \n",
    "    if CONFIG['save_results']:\n",
    "        print(f\"üíæ Saved to: {CONFIG['output_catalog']}.{CONFIG['output_schema']} schema\")\n",
    "    \n",
    "    print(f\"üéØ Views Created: Use SQL queries on dlt_consolidated_events and dlt_* views\")\n",
    "    \n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "print(f\"   1. Query pipeline runs by name: SELECT * FROM alex_linke.wesco.dlt_pipeline_runs WHERE pipeline_name = 'YourPipelineName'\")\n",
    "print(f\"   2. Monitor performance: SELECT pipeline_name, success_rate, avg_duration_minutes FROM alex_linke.wesco.dlt_performance_metrics\")\n",
    "print(f\"   3. Build dashboards using the saved Delta tables with actual pipeline names and run metadata\")\n",
    "print(f\"   4. Set up alerts on failure_rate, avg_duration spikes, or error patterns\")\n",
    "print(f\"   5. Schedule this script to run regularly for ongoing pipeline monitoring\")\n",
    "\n",
    "print(f\"\\nüîö Script completed successfully!\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6444096297414630,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "jiig-ldp-analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
